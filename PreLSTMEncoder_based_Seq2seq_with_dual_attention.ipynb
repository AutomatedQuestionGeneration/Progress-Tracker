{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "new-pipelin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfbpjs2fgzuT",
        "outputId": "37a1dfd3-1b03-4061-95ed-8028900e980d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTBFuHj0u3sC"
      },
      "source": [
        "# SQUAD DEV SET ANALYSIS "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2ApHDPLj22f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjF9hLAFu7Jr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "1c2c00cc-405d-4711-adae-f45bd58cd5ad"
      },
      "source": [
        "dev = pd.read_csv(\"/content/drive/MyDrive/AutomatedQuestionGeneration/Topic_Bifurcated_dev.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ac0a2dbbbd27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/AutomatedQuestionGeneration/Topic_Bifurcated_dev.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AutomatedQuestionGeneration/Topic_Bifurcated_dev.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaebmHR6u7Hj"
      },
      "source": [
        "dev.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y1Cakuyvk04"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Rm5SrOvwRD"
      },
      "source": [
        "dev = dev.drop_duplicates(subset = ['context'],keep = 'first').reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2398rpzv6pS"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Lr8kejxxvj",
        "outputId": "99e454ba-da5c-4581-ea01-fb21051b1743"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "3IeB_gDadfhr",
        "outputId": "5b064ce8-533e-40d9-ce95-3bdf5143b03c"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab_data/Topic_Bifurcated_SQUAD1.csv\")\n",
        "df = df.drop_duplicates(subset = ['given_context', 'reference_context'],keep = 'first').reset_index(drop = True)\n",
        "# df = df[:100]\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be86cf3aeaaa14008c9076</td>\n",
              "      <td>After her second solo album, what other entert...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>acting</td>\n",
              "      <td>207.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6e823aeaaa14008c962a</td>\n",
              "      <td>Which album was darker in tone from her previo...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ...                                  reference_context\n",
              "0  56be85543aeaaa14008c9063  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1  56be85543aeaaa14008c9065  ...  Born and raised in Houston, Texas, she perform...\n",
              "2  56bf6b0f3aeaaa14008c9605  ...  Managed by her father, Mathew Knowles, the gro...\n",
              "3  56be86cf3aeaaa14008c9076  ...  Beyoncé also ventured into acting, with a Gold...\n",
              "4  56bf6e823aeaaa14008c962a  ...  Following the disbandment of Destiny's Child i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOsMShvlg4ua"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load data set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeHAlqRBeaPJ"
      },
      "source": [
        "# Train Set\n",
        "reference_context = df[\"reference_context\"].apply(lambda x: x).tolist()\n",
        "given_context = df[\"given_context\"].apply(lambda x: x).tolist()\n",
        "target_ques = df[\"question\"].apply(lambda x: x).tolist()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8SYzSr-xFiT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "65708b7a-cfcb-4ac3-816e-995207979354"
      },
      "source": [
        "# Test Set\n",
        "reference_context = dev[\"reference_context\"].apply(lambda x: x).tolist()\n",
        "given_context = dev[\"given_context\"].apply(lambda x: x).tolist()\n",
        "target_ques = dev[\"question\"].apply(lambda x: x).tolist()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a5af10df7a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreference_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reference_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgiven_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"given_context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_ques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDa3tH1hRyNq"
      },
      "source": [
        "*   Clean the sentences by removing special characters.\n",
        "*   Add a start and end token to each sentence.\n",
        "*   Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "*   Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKjEWl6WhZyk"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(str(w).lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\",\"¿\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  # remove extra space\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc58-K0XhdCM",
        "outputId": "05769900-b046-491e-f411-50f95a1bb172"
      },
      "source": [
        "en_sentence = u\"Beyonce's last record was called Lemonade\"\n",
        "sp_sentence = u\"Beyonce was born in 1950\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode(\"UTF-8\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> beyonce s last record was called lemonade <end>\n",
            "<start> beyonce was born in <end>\n",
            "b'<start> beyonce was born in <end>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYdaxYI2df3H",
        "outputId": "77e30df0-86d5-4954-f543-51e0be25ef0c"
      },
      "source": [
        "reference_context[:1], given_context[:1]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". '],\n",
              " [\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \"])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hw9ct4OhsRA",
        "outputId": "67fff2bc-541c-4fa7-9318-56ef5815c51b"
      },
      "source": [
        "# def create_dataset_squad(context_param, given_param, target_ques):\n",
        "  # reference_context = []\n",
        "  # given_context = []\n",
        "  # question = []\n",
        "  # for c,g,q in zip(context_param,given_param, target_ques):\n",
        "    # reference_context.append(preprocess_sentence(str(c)))\n",
        "    # given_context.append(preprocess_sentence(str(g)))\n",
        "    # question.append(preprocess_sentence(q))\n",
        "  # return tuple(reference_context), tuple(given_context), tuple(question)\n",
        "\n",
        "# r, g, q = create_dataset_squad(reference_context, given_context, target_ques)\n",
        "print(r[-1])\n",
        "print(g[-1])\n",
        "print(q[-1])\n",
        "print(len(r), len(q))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this activity has been further enhanced by establishing formal relationships with other cities motsumoto city of japan , rochester of the usa , yangon formerly rangoon of myanmar , xi an of the people s republic of china , minsk of belarus , and pyongyang of the democratic republic of korea . kmc s constant endeavor is to enhance its interaction with saarc countries , other international agencies and many other major cities of the world to achieve better urban management and developmental programs for kathmandu . <end>\n",
            "<start> kathmandu metropolitan city kmc , in order to promote international relations has established an international relations secretariat irc . kmc s first international relationship was established in with the city of eugene , oregon , united states . <end>\n",
            "<start> what was yangon previously known as ? <end>\n",
            "43275 43275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCAYRuTlh5DY"
      },
      "source": [
        "# Tokenize the sentence into list of words(integers) and pad the sequence to the same length\n",
        "# def tokenize(lang):\n",
        "#   lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "#       filters='')\n",
        "#   lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "#   tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "#   tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "#                                                          padding='post')\n",
        "#   return tensor, lang_tokenizer"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTX167EEkUWI"
      },
      "source": [
        "df = df[:100]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM87LBJ4ewey"
      },
      "source": [
        "# def load_dataset(reference, given, target):\n",
        "#   # creating cleaned input, output pairs\n",
        "#   #targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "#   reference_tensor, reference_tokenizer = tokenize(reference)\n",
        "#   given_tensor, given_tokenizer = tokenize(given)\n",
        "#   target_tensor, targ_lang_tokenizer = tokenize(target)\n",
        "\n",
        "#   return reference_tensor, given_tensor, target_tensor, reference_tokenizer, given_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EasB_FLig5c"
      },
      "source": [
        "# # Try experimenting with the size of that dataset\n",
        "# #num_examples = 30000\n",
        "# # input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(c,q)\n",
        "# reference_tensor, given_tensor, target_tensor, reference_tokenizer, given_tokenizer, targ_tokenizer = load_dataset(r, g, q)\n",
        "\n",
        "# # Calculate max_length of the target tensors\n",
        "# max_length_targ, max_length_reference, max_length_given = target_tensor.shape[1], reference_tensor.shape[1], given_tensor.shape[1]\n",
        "# print(max_length_targ, max_length_reference, max_length_given)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMgiyh8igJ-3"
      },
      "source": [
        "# train = zip(reference_tensor, given_tensor)\n",
        "# train = pd.DataFrame(data=train, index=None)\n",
        "# train.head()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxsx7boamIAa",
        "outputId": "6ee67bcc-604a-4932-8aaa-b3da434db085"
      },
      "source": [
        "context = df[\"context\"].apply(lambda x: x).tolist()\n",
        "target_ques = df[\"question\"].apply(lambda x: x).tolist()\n",
        "def create_dataset_squad(context_param,target_ques):\n",
        "  context = []\n",
        "  question = []\n",
        "  for c,q in zip(context_param,target_ques):\n",
        "    context.append(preprocess_sentence(c))\n",
        "    question.append(preprocess_sentence(q))\n",
        "  return context, question\n",
        "\n",
        "c, q = create_dataset_squad(context,target_ques)\n",
        "print(c[-1])\n",
        "print(q[-1])\n",
        "print(len(c), len(q))\n",
        "# Tokenize the sentence into list of words(integers) and pad the sequence to the same length\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer\n",
        "def load_dataset(input, target):\n",
        "  # creating cleaned input, output pairs\n",
        "  #targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(input)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(target)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "# Try experimenting with the size of that dataset\n",
        "#num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(c,q)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print(max_length_targ, max_length_inp)\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "print(input_tensor_train[1])\n",
        "print()\n",
        "print(target_tensor_train[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> beyonce and husband jay z are friends with president barack obama and first lady michelle obama . she performed america the beautiful at the presidential inauguration , as well as at last during the first inaugural dance at the neighborhood ball two days later . beyonce and jay z held a fundraiser at the latter s club in manhattan for obama s presidential campaign which raised million . beyonce uploaded pictures of her paper ballot on tumblr , confirming she had voted in support for the democratic party and to encourage others to do so . she also performed the american national anthem at his second inauguration , singing along with a pre recorded track . she publicly endorsed same sex marriage on march , , after the supreme court debate on california s proposition . in july , beyonce and jay z attended a rally in response to the acquittal of george zimmerman for the shooting of trayvon martin . <end>\n",
            "<start> how much did beyonce raise for obama at the club ? <end>\n",
            "100 100\n",
            "21 358\n",
            "80 80 20 20\n",
            "[  22    4    1 1129 1130   29 1131  566   55    6   18   31    9 1132\n",
            "  452  256   67   13    2  164    9 1133 1134 1135 1136    3 1137 1138\n",
            "  604   55    2   33  267   18 1139  140   12  473    2  843  163  195\n",
            "   13    2  324    7 1140   13    6   81 1141   12    2 1142 1143   55\n",
            "   19  428    2  843   12    2 1144 1145 1146 1147    3   81   55   59\n",
            "   19   78    2   20   79  106   68   12 1148    2  474 1149  307   15\n",
            "    2 1150  298    4  120  463  148    1    5   18  453    2 1151 1152\n",
            " 1153    4    2  104  299  475    3   23    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "\n",
            "[  1  14   7   4 279   5 105  20 280  24  12 281   2   3   0   0   0   0\n",
            "   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABLcWxVkmNbS",
        "outputId": "860159bc-1d92-4fbd-bb68-209d3ea0ce91"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "steps_per_epoch_val = len(input_tensor_val)//BATCH_SIZE\n",
        "embedding_dim = 256  # for word embedding\n",
        "units = 1024  # dimensionality of the output space of RNN\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 358]), TensorShape([16, 21]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oS6mqluirWG"
      },
      "source": [
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(train, target_tensor, test_size=0.2)\n",
        "# reference_tensor_train, reference_tensor_val, given_tensor_train, given_tensor_val, target_tensor_train, target_tensor_val = train_test_split(reference_tensor, given_tensor, target_tensor,random_state = 101)\n",
        "\n",
        "# Show length\n",
        "# print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "# print(input_tensor_train[1])\n",
        "# print()\n",
        "# print(target_tensor_train[1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKWru44VgqfN"
      },
      "source": [
        "# input_tensor_train\n",
        "# reference_tensor_train = input_tensor_train[0].values\n",
        "# given_tensor_train = input_tensor_train[1].values\n",
        "# \n",
        "# reference_tensor_val = input_tensor_val[0].values\n",
        "# given_tensor_val = input_tensor_val[1].values"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK3NE1vUityv"
      },
      "source": [
        "# # Configuration \n",
        "# BUFFER_SIZE = len(reference_tensor_train)\n",
        "# BATCH_SIZE = 16\n",
        "# steps_per_epoch = len(reference_tensor_train)//BATCH_SIZE\n",
        "# steps_per_epoch_val = len(reference_tensor_val)//BATCH_SIZE\n",
        "# embedding_dim = 256  # for word embedding\n",
        "# units = 1024  # dimensionality of the output space of RNN\n",
        "# vocab_inp_size = len(given_tokenizer.word_index) + len(reference_tokenizer.word_index) + 1 \n",
        "\n",
        "# vocab_tar_size = len(targ_tokenizer.word_index) + 1\n",
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((reference_tensor_train, given_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "# validation_dataset = tf.data.Dataset.from_tensor_slices((reference_tensor_val, given_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "# validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# example_reference_batch, example_given_batch, example_target_batch = next(iter(dataset))\n",
        "# example_reference_batch.shape, example_given_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzo3ZvYpv4KE"
      },
      "source": [
        "# Seq2seq model: Two Encoders and DecoderWithDualAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFjq9Ta3wA8F"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.LSTM = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,  # Whether to return the last output in the output sequence, or the full sequence. \n",
        "                                   return_state=True,  # Whether to return the last state in addition to the output.\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(Encoder, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'batch_sz': self.batch_sz,\n",
        "            'enc_units': self.enc_units,\n",
        "            # 'embedding': self.embedding,\n",
        "            # 'LSTM': self.LSTM,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embedding_dim': self.embedding_dim\n",
        "        })\n",
        "        return config \n",
        "\n",
        "  \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, hidden_h, hidden_c = self.LSTM(x, initial_state = hidden)\n",
        "    return output, [hidden_h,hidden_c]\n",
        "\n",
        "  @tf.function\n",
        "  def initialize_hidden_state(self):\n",
        "    init_state = [tf.zeros([self.batch_sz, self.enc_units]) for i in range(2)]\n",
        "    return init_state"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4GoRSHSwScu",
        "outputId": "08de2ee0-9af8-46d5-a4dd-608c695be8f2"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "#sample_hidden = [tf.zeros([1,1024]) for i in range(2)]\n",
        "sample_output, sample_hidden= encoder(example_reference_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
        "print ('Encoder Carry state shape: (batch size, units) {}'.format(sample_hidden[1].shape))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (16, 452, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (16, 1024)\n",
            "Encoder Carry state shape: (batch size, units) (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSWViCN635g8"
      },
      "source": [
        "# Additive attention\n",
        "\n",
        "![alt text](https://i.ibb.co/BqDYNP1/additive.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXoFjEEoVPlI"
      },
      "source": [
        "#tf.keras.layers.Layer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOzyie8DLjEI"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    self.units = units\n",
        "    super(BahdanauAttention, self).__init__(**kwargs)\n",
        "\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(BahdanauAttention, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'units': self.units,   \n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  \n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "    # print(\"Query : ,\",query)\n",
        "    \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    # print(\"Query with time axis: ,\",query_with_time_axis.shape)\n",
        "    # query_with_time_axis = query\n",
        "    # values = query_with_time_axis\n",
        "    # print(\"Values: ,\",values.shape)\n",
        "    # print(values.shape)\n",
        "    # print(query_with_time_axis.shape)\n",
        "\n",
        "    # (64, 358, 1024) ->values.shape\n",
        "    # (64, 1, 1024) -> query_with_time_axis.shape\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(query_with_time_axis)))\n",
        "    # print(\"W1(Values): \", self.W1(values).shape)\n",
        "    # print(\"W2(Query_with_time_axis): \", self.W2(query_with_time_axis).shape)\n",
        "    # print(\"Score: \", score.shape)\n",
        "    # (64, 358, 1024) -> self.W1(values).shape\n",
        "    # (64, 1, 1024) -> self.W2(query_with_time_axis).shape\n",
        "    # (64, 358, 1) -> score.shape\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    # print(context_vector.shape)\n",
        "    # (64, 358, 1024)\n",
        "\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    # print(context_vector.shape)\n",
        "    # (64, 1024)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNsmnspoSbz8"
      },
      "source": [
        "class DecoderWithDualAttention(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_layer1 = None, attention_layer2 = None):\n",
        "    super(DecoderWithDualAttention, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.LSTM = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "    # used for attention\n",
        "    self.attention1 = attention_layer1\n",
        "    self.attention2 = attention_layer2\n",
        "\n",
        "  \n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(DecoderWithDualAttention, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'batch_sz': self.batch_sz,\n",
        "            'dec_units': self.dec_units,\n",
        "            # 'embedding': self.embedding,\n",
        "            # 'LSTM': self.LSTM,\n",
        "            # 'fc' : self.fc,\n",
        "            'attention_layer1':self.attention1,\n",
        "            'attention_layer2':self.attention2,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embedding_dim': self.embedding_dim\n",
        "        })\n",
        "        return config\n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "  def call(self, x, hidden1, enc_output1):\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    # y = x\n",
        "    attention_weights1 = None\n",
        "    output1, state_h1, state_c1 = self.LSTM(x, initial_state = hidden1)\n",
        "    print(\"output1:\", output1.shape)\n",
        "    print(\"state_h1:\", state_h1.shape)\n",
        "    # Attention 1 is used to associate the target word embedding with the reference context embedding (attended)\n",
        "    if self.attention1:\n",
        "      # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "      context_vector1, attention_weights1 = self.attention1(output1, enc_output1)\n",
        "      print(\"context_vector1:\", context_vector1.shape)\n",
        "      # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "      # output1_context_vector1 = tf.concat([context_vector1, output1], axis=-1)\n",
        "      output1_context_vector1 = tf.concat([context_vector1, output1], axis=-1)\n",
        "      output1_context_vector1 = tf.transpose(ooutput1_context_vector1, perm = [0, 2, 1])\n",
        "      # x = tf.concat([context_vector1, x], axis=-1)\n",
        "      print(\"output1_context_vector1:\", output1_context_vector1.shape)\n",
        "    # attention_weights2 = None\n",
        "    \n",
        "    # Attention 2 is used to associate the target word embedding with the given context embedding (attended)\n",
        "    # if self.attention2:\n",
        "    #   # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    #   context_vector2, attention_weights2 = self.attention2(hidden2[0], enc_output2)\n",
        "    #   # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    #   y = tf.concat([tf.expand_dims(context_vector2, 1), y], axis=-1)\n",
        "    #   # y = tf.concat([context_vector2, y], axis=-1)\n",
        "  \n",
        "    attention_vector = self.Wc(output1_context_vector1)\n",
        "    print(\"attention_vector:\", attention_vector.shape)\n",
        "    # passing the concatenated vector to the LSTM\n",
        "    # output2, state_h2, state_c2= self.LSTM(y, initial_state = hidden2)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    # output1 = tf.reshape(output1, (-1, output1.shape[2]))\n",
        "    # output2 = tf.reshape(output2, (-1, output2.shape[2]))\n",
        "\n",
        "    # output = tf.concat([output1, output2], axis = 1)\n",
        "    \n",
        "    # output shape == (batch_size, vocab)\n",
        "    final_output = self.fc(attention_vector)\n",
        "    \n",
        "    # return final_output, [state_h1,state_c1], [state_h2,state_c2], attention_weights1, attention_weights2\n",
        "    return final_output, [state_h1,state_c1], [1,1], attention_weights1, attention_weights1"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oozBiV1Khj"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFLLiTBgnwG-",
        "outputId": "cec37e52-3fbc-413e-e4c3-9c4ff56bb3a0"
      },
      "source": [
        "print(loss_object([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))\n",
        "print(loss_function([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1.063386  1.3633859], shape=(2,), dtype=float32)\n",
            "tf.Tensor(1.2133859, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz6c3l931TC8"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def get_train_step_func():\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(inp, targ, enc_hidden1, encoder1, decoder):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape: # for automatic differentiation\n",
        "      enc_output1, enc_hidden1 = encoder1(inp, enc_hidden1)\n",
        "      # enc_output2, enc_hidden2 = encoder2(aux, enc_hidden2)\n",
        "\n",
        "      # Initialising decoder hidden states with the corresponding hidden states of the encoder (one for given context encoder and the other for reference context encoder)\n",
        "      dec_hidden1 = enc_hidden1\n",
        "      # dec_hidden2 = enc_hidden2\n",
        "      # enc_hidden -> \n",
        "\n",
        "      # Check whether we should concatenate dec_hidden and dec_hidden2 or keep it separate\n",
        "      # dec_hidden2 = enc_hidden2\n",
        "\n",
        "      dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      # Teacher forcing - feeding the target as the next input\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        # print(\"Iteration: \",t)\n",
        "        # passing enc_output to the decoder\n",
        "        # print(\"EncOutput1 before: \",enc_output1)\n",
        "        predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, enc_output1)\n",
        "        # print(\"EncOutput1 after: \",enc_output1)\n",
        "        \n",
        "        # x, y, hidden1, hidden2, enc_output1, enc_output2\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder1.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "    \n",
        "  return train_step"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zym-buZ_TEg"
      },
      "source": [
        "def caculate_validation_loss(inp, targ, enc_hidden1, encoder1, decoder):\n",
        "  loss = 0\n",
        "  enc_output1, enc_hidden1 = encoder1(inp, enc_hidden1)\n",
        "  # enc_output2, enc_hidden2 = encoder2(aux, enc_hidden1)\n",
        "  dec_hidden1 = enc_hidden1\n",
        "  # dec_hidden2 = enc_hidden2\n",
        "  dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "  # Teacher forcing - feeding the target as the next input\n",
        "  # for t in range(1, targ.shape[1]):\n",
        "  #   predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "  #   loss += loss_function(targ[:, t], predictions)\n",
        "  #   dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # loss = loss / int(targ.shape[1])\n",
        "  # return loss\n",
        "\n",
        "  for t in range(1, targ.shape[1]):\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, enc_output1)\n",
        "        # x, y, hidden1, hidden2, enc_output1, enc_output2\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  loss = loss / int(targ.shape[1])\n",
        "  return loss"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMo0_PVaJiP7"
      },
      "source": [
        " def training_seq2seq(epochs, attention1):\n",
        "  encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  # encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)\n",
        "  train_step_func = get_train_step_func()\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print()\n",
        "    print(epoch)\n",
        "    start = time.time()\n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    # enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      print(batch)\n",
        "      batch_loss = train_step_func(inp, targ, enc_hidden1, encoder1, decoder)\n",
        "      # inp, targ, aux, enc_hidden, enc_hidden2, encoder, encoder2, decoder\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "        \n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    # enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_val_loss = 0\n",
        "    for (batch, (inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch)):\n",
        "      val_loss = caculate_validation_loss(inp, targ, enc_hidden1, encoder1, decoder)\n",
        "      total_val_loss += val_loss\n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder1, 1, decoder, training_loss, validation_loss"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4N30XinpVY"
      },
      "source": [
        "# Training Starts Here"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPmQAjf9ULa"
      },
      "source": [
        "def resume_training_seq2seq(epochs, attention1, attention2):\n",
        "  encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)\n",
        "  encoder1.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder1-10\")\n",
        "  encoder2.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder2-10\")\n",
        "  decoder.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/decoder10\")\n",
        "  print(\"\\n\\nLoading done\\n\\n\")\n",
        "  train_step_func = get_train_step_func()\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print()\n",
        "    print(epoch)\n",
        "    start = time.time()\n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (aux, inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      print(batch)\n",
        "      batch_loss = train_step_func(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      # inp, targ, aux, enc_hidden, enc_hidden2, encoder, encoder2, decoder\n",
        "      total_loss += batch_loss\n",
        "      print(\"\\n\\nBatch done\\n\\n\")\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "        \n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_val_loss = 0\n",
        "    for (batch, (aux, inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch)):\n",
        "      val_loss = caculate_validation_loss(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      total_val_loss += val_loss\n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder1, encoder2, decoder, training_loss, validation_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKjqurFVY0u"
      },
      "source": [
        "## Training seq2seq with Bahdanau attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUKivtyYix6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "e252a3a1-3e2f-4179-ba91-35575d0c6f80"
      },
      "source": [
        "epochs = 1\n",
        "\n",
        "attention1 = BahdanauAttention(units)\n",
        "# attention2 = BahdanauAttention(units)\n",
        "print(\"Running seq2seq model with Bahdanau attention\")\n",
        "encoder1_bah, encoder2_bah2, decoder_bah, training_loss, validation_loss = training_seq2seq(epochs, attention1)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running seq2seq model with Bahdanau attention\n",
            "\n",
            "0\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Encoder.initialize_hidden_state at 0x7f2268498f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0\n",
            "output1: (16, 1, 1024)\n",
            "state_h1: (16, 1024)\n",
            "context_vector1: (16, 358, 1024)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-cc17d1f261e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# attention2 = BahdanauAttention(units)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running seq2seq model with Bahdanau attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoder1_bah\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder2_bah2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_bah\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-52bc6b5cb4b5>\u001b[0m in \u001b[0;36mtraining_seq2seq\u001b[0;34m(epochs, attention1)\u001b[0m\n\u001b[1;32m     17\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m      \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m      \u001b[0;31m# inp, targ, aux, enc_hidden, enc_hidden2, encoder, encoder2, decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m      \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-50-6a7d9ecf3dde>:28 train_step  *\n        predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, enc_output1)\n    <ipython-input-69-3f682a10074f>:56 call  *\n        output1_context_vector1 = tf.concat([context_vector1, output1], axis=-1)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1769 concat\n        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:1228 concat_v2\n        \"ConcatV2\", values=values, axis=axis, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension 1 in both shapes must be equal, but are 358 and 1. Shapes are [16,358] and [16,1]. for '{{node decoder_with_dual_attention_3/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](decoder_with_dual_attention_3/bahdanau_attention_5/Sum, decoder_with_dual_attention_3/lstm_9/PartitionedCall:1, decoder_with_dual_attention_3/concat/axis)' with input shapes: [16,358,1024], [16,1,1024], [] and with computed input tensors: input[2] = <-1>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4z2MxmGq329"
      },
      "source": [
        "a = tf.ones((16, 358, 1024))\n",
        "b = tf.zeros((16, 1, 1024))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "WPjzerBmq93s",
        "outputId": "a50e07c3-e459-4543-d8fd-4c1075ae61a4"
      },
      "source": [
        "tf.concat([a, b], axis=-1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-4a18f08e45e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1767\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1768\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [16,358,1024] vs. shape[1] = [16,1,1024] [Op:ConcatV2] name: concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eev4IrPIZj-S"
      },
      "source": [
        "tf.keras.models.save_model(encoder1_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/encoder1-final\")\n",
        "tf.keras.models.save_model(encoder2_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/encoder2-final\")\n",
        "tf.keras.models.save_model(decoder_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/decoder-final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veo1kmoOCNWD"
      },
      "source": [
        "# Loading Trained Model\n",
        "attention1 = BahdanauAttention(units)\n",
        "attention2 = BahdanauAttention(units)\n",
        "encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlX_NB9fQlEY",
        "outputId": "bab43d5b-8e9c-40ad-c72d-09870a37491b"
      },
      "source": [
        "encoder1 = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/encoder1-final/\")\n",
        "encoder2 = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/encoder2-final\")\n",
        "decoder = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/decoder-final/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9Fn5HrOO6i"
      },
      "source": [
        "def translate(g_context, r_context, encoder1, encoder2, decoder):\n",
        "  attention_plot1 = np.zeros((max_length_targ, max_length_given))\n",
        "  attention_plot2 = np.zeros((max_length_targ, max_length_reference))\n",
        "\n",
        "  g_context = preprocess_sentence(g_context)\n",
        "\n",
        "  given = [given_tokenizer.word_index[i] for i in g_context.split(' ')]\n",
        "  given = tf.keras.preprocessing.sequence.pad_sequences([given],\n",
        "                                                         maxlen=max_length_given,\n",
        "                                                         padding='post')\n",
        "  # given = tf.convert_to_tensor(given)\n",
        "  given = tf.reshape(tf.convert_to_tensor(given),shape = [-1,max_length_given])\n",
        "\n",
        "  \n",
        "  r_context = preprocess_sentence(r_context)\n",
        "  \n",
        "  ref = [reference_tokenizer.word_index[i] for i in r_context.split(' ')]\n",
        "  ref = tf.keras.preprocessing.sequence.pad_sequences([ref],\n",
        "                                                         maxlen=max_length_reference,\n",
        "                                                         padding='post')\n",
        "  # ref = tf.convert_to_tensor(ref)\n",
        "  ref = tf.reshape(tf.convert_to_tensor(ref),shape = [-1,max_length_reference])\n",
        "\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units)) for i in range(2)]\n",
        "  # hidden = encoder1.initialize_hidden_state()\n",
        "  enc_out1, enc_hidden1 = encoder1(given, hidden)\n",
        "  enc_out2, enc_hidden2 = encoder2(ref, hidden)\n",
        "\n",
        "  dec_hidden1 = enc_hidden1\n",
        "  dec_hidden2 = enc_hidden2\n",
        "  dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    # predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "    #                                                      dec_hidden,\n",
        "    #                                                      enc_out)\n",
        "\n",
        "    predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, dec_hidden2, enc_out1, enc_out2)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy() \n",
        "    # print(predicted_id)\n",
        "\n",
        "    result += targ_tokenizer.index_word[predicted_id] + ' '\n",
        "    # print(result)\n",
        "\n",
        "    # until the predicted word is <end>.\n",
        "    if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result\n",
        "\n",
        "    # the predicted ID is fed back into the model, no teacher forcing.\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "hkePMDa2weMS",
        "outputId": "9228222b-5a7a-4016-f91e-f01879941d0b"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>249</td>\n",
              "      <td>Carolina Panthers</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>403</td>\n",
              "      <td>Santa Clara, California</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>355</td>\n",
              "      <td>Levi's Stadium</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "3  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "6  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "7  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XOdCTw9Izk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a943d065-111a-467f-ae49-a02e3a53cac5"
      },
      "source": [
        "translate(dev.iloc[10,5],dev.iloc[10,6], encoder1, encoder2, decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what was the first year that the cia s budget was disclosed ? <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2M_D13_T3mrD",
        "outputId": "e2a36c67-6fe5-4712-e850-3dcfdd68a02b"
      },
      "source": [
        "dev.iloc[10,5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. \""
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "d5GnhZZ63oga",
        "outputId": "c145f80d-88ec-4b9d-ff4d-8e05d3c77023"
      },
      "source": [
        "dev.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. '"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qfwxeCpo9JkB",
        "outputId": "b33fa887-229c-44b8-cdd2-cb4dc029653f"
      },
      "source": [
        "dev.iloc[10,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the theme of Super Bowl 50?'"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "SZqH2rnW3qwc",
        "outputId": "d6348c4d-0da2-4fb1-ebe8-b2bd1857ba01"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>249</td>\n",
              "      <td>Carolina Panthers</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>403</td>\n",
              "      <td>Santa Clara, California</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>355</td>\n",
              "      <td>Levi's Stadium</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "3  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "6  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "7  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d2QW9XnpIWW",
        "outputId": "7e94690b-a8fe-49c0-c6b2-fdfb3a87278e"
      },
      "source": [
        "try:\n",
        "  translate(df.iloc[8578,6],df.iloc[8578,7], encoder1, encoder2, decoder)\n",
        "except:\n",
        "  print(\"Error\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Im3qC71U2e7a",
        "outputId": "8898a46f-7ee0-49d2-cd49-23ff582ba7f5"
      },
      "source": [
        "df.iloc[10000,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. \""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0r37uLAm2hyI",
        "outputId": "1d6275c7-55b8-4f4d-d5ac-ed79ba8a2a4a"
      },
      "source": [
        "df.iloc[10000,7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman. '"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mBlYXuZv6kvM",
        "outputId": "f7db1746-04b8-4024-bb13-8dc617bfa72e"
      },
      "source": [
        "df.iloc[10100,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Postmarital residence among hunter-gatherers tends to be matrilocal, at least initially. Young mothers can enjoy childcare support from their own mothers, who continue living nearby in the same camp. '"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nN8_InQ4D3k"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzXHiKQhNkRS"
      },
      "source": [
        "predict_df = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/LSTMpredictions17861.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mThIgZsa3Wew",
        "outputId": "5d10f29b-f54e-4b4b-d8e2-2a388f0060e2"
      },
      "source": [
        "predict_df = df[[\"given_context\",\"reference_context\"]]\n",
        "predict_df.columns = [\"given_context\",\"reference_context\"]\n",
        "predict_df[\"generated_question\"] = 'a'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkmSbsPH4mwh",
        "outputId": "2b5daae1-645d-48dd-a890-19fa59bbe5f6"
      },
      "source": [
        "for i in range(8570,8580):\n",
        "  try:\n",
        "    predict_df[\"generated_question\"][i] = translate(predict_df[\"given_context\"][i], predict_df[\"reference_context\"][i],encoder1,encoder2,decoder)\n",
        "  except:\n",
        "    predict_df[\"given_context\"][i], predict_df[\"reference_context\"][i] = np.nan,np.nan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AWU4goD-I5u"
      },
      "source": [
        "predict_df[8570:8581].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qmkJ1vvo8o1Y",
        "outputId": "813c8e75-fe41-432a-e4fc-151a83b8be2b"
      },
      "source": [
        "predict_df[8570:8581]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "      <th>generated_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8570</th>\n",
              "      <td>According to Y chromosome studies by Sanchez e...</td>\n",
              "      <td>(2004, 2007), the Somalis are paternally close...</td>\n",
              "      <td>what is the most common surname in the italian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8571</th>\n",
              "      <td>(2005), Cruciani et al. Sanchez et al. Accordi...</td>\n",
              "      <td>According to Y chromosome studies by Sanchez e...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8572</th>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8573</th>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>This mitochondrial clade is common among Ethio...</td>\n",
              "      <td>what is the opposite of blackness ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8574</th>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8575</th>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>what is the most common soundboard shape ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8576</th>\n",
              "      <td>According to an autosomal DNA study by Hodgson...</td>\n",
              "      <td>(2014), the Afro-Asiatic languages were likely...</td>\n",
              "      <td>what was the landline growth rate of somalia i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8577</th>\n",
              "      <td>According to an autosomal DNA study by Hodgson...</td>\n",
              "      <td>(2014), the Afro-Asiatic languages were likely...</td>\n",
              "      <td>what is the posulated homeland region of the r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8578</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8579</th>\n",
              "      <td>The history of Islam in Somalia is as old as t...</td>\n",
              "      <td>The early persecuted Muslims fled to various p...</td>\n",
              "      <td>what is the name of the nomad tribes that hunt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8580</th>\n",
              "      <td>The early persecuted Muslims fled to various p...</td>\n",
              "      <td>The history of Islam in Somalia is as old as t...</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          given_context  ...                                 generated_question\n",
              "8570  According to Y chromosome studies by Sanchez e...  ...  what is the most common surname in the italian...\n",
              "8571  (2005), Cruciani et al. Sanchez et al. Accordi...  ...  what is the most efficient means of transporta...\n",
              "8572  According to mtDNA studies by Holden (2005) an...  ...  what is the most efficient means of transporta...\n",
              "8573  According to mtDNA studies by Holden (2005) an...  ...         what is the opposite of blackness ? <end> \n",
              "8574  (2006), a significant proportion of the matern...  ...  what is the most efficient means of transporta...\n",
              "8575  (2006), a significant proportion of the matern...  ...  what is the most common soundboard shape ? <end> \n",
              "8576  According to an autosomal DNA study by Hodgson...  ...  what was the landline growth rate of somalia i...\n",
              "8577  According to an autosomal DNA study by Hodgson...  ...  what is the posulated homeland region of the r...\n",
              "8578                                                NaN  ...                                                  a\n",
              "8579  The history of Islam in Somalia is as old as t...  ...  what is the name of the nomad tribes that hunt...\n",
              "8580  The early persecuted Muslims fled to various p...  ...                                                  a\n",
              "\n",
              "[11 rows x 3 columns]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jrc-XOtkH_6P",
        "outputId": "6bb0eb4b-8697-4370-be02-d0eb6eae7bb6"
      },
      "source": [
        "#Given_Context\n",
        "df.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PmPeKfbsH_4A",
        "outputId": "25013eba-8d77-4366-e882-d81c9d17484b"
      },
      "source": [
        "#Reference_Context\n",
        "df.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-0zNw7T3FmXm",
        "outputId": "5f716030-2a62-4241-8af5-5b1e9116bdf6"
      },
      "source": [
        "predict_df.iloc[10,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'when did beyonce start becoming a golden globe ? <end> '"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CYjfp027F9vZ",
        "outputId": "080f3043-e0d6-467d-e775-b85b884ca048"
      },
      "source": [
        "df.iloc[10,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the first album Beyoncé released as a solo artist?'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRsovWZukUlm",
        "outputId": "421ae3aa-54d7-49af-88f8-64237399d2fb"
      },
      "source": [
        "import math\n",
        "math.isnan(df[\"reference_context\"][87])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Pa6vIklC4l"
      },
      "source": [
        "pd.isnull(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gpnDtMIt2QOL",
        "outputId": "a1aaf2d6-2760-4b69-bb59-37d8ca22caa3"
      },
      "source": [
        "squad.iloc[8578,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'After August 2008 sites had to be listed on the Open Directory in order to be included. According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived, but more recent captures would become visible only after the next major indexing, an infrequent operation.'"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2jQN-LOZkMLr",
        "outputId": "bec89535-41f0-4265-8171-cd39cb537f76"
      },
      "source": [
        "df.iloc[8578,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'According to Mohamoud et al. '"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cvg2b1fh0V4r",
        "outputId": "930833ea-d79b-46b9-fb5a-3753eb214000"
      },
      "source": [
        "df.iloc[8578,7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(2006): '"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "5ISyfcYxYlg-",
        "outputId": "ace6c454-8164-4065-8a06-7777b5592989"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be86cf3aeaaa14008c9076</td>\n",
              "      <td>After her second solo album, what other entert...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>acting</td>\n",
              "      <td>207.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6e823aeaaa14008c962a</td>\n",
              "      <td>Which album was darker in tone from her previo...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ...                                  reference_context\n",
              "0  56be85543aeaaa14008c9063  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1  56be85543aeaaa14008c9065  ...  Born and raised in Houston, Texas, she perform...\n",
              "2  56bf6b0f3aeaaa14008c9605  ...  Managed by her father, Mathew Knowles, the gro...\n",
              "3  56be86cf3aeaaa14008c9076  ...  Beyoncé also ventured into acting, with a Gold...\n",
              "4  56bf6e823aeaaa14008c962a  ...  Following the disbandment of Destiny's Child i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0EhKrkbWXhJ"
      },
      "source": [
        "newdf = df.drop_duplicates(\n",
        "  subset = ['context'],\n",
        "  keep = 'first').reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQsgNsz2Wu8G"
      },
      "source": [
        "newdf.to_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/UniqueContextBifurcated.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cv59HGDF0i"
      },
      "source": [
        "predict_df.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbR-CA1WIscZ",
        "outputId": "8be77c73-8374-45d4-dcf3-4ba34c2bd881"
      },
      "source": [
        "index = int(input(\"Enter record index: \"))\n",
        "#Given_Context\n",
        "print(\"Given context:\\n\",predict_df.iloc[index,0])\n",
        "#Reference_Context\n",
        "print(\"\\nReference context:\\n\",predict_df.iloc[index,1])\n",
        "# Generated Question\n",
        "print(\"\\nGenerated question: \",predict_df.iloc[index,3])\n",
        "# Target Question\n",
        "print(\"\\nTarget question: \",predict_df.iloc[index,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter record index: 0\n",
            "Given context:\n",
            " Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \n",
            "\n",
            "Reference context:\n",
            " Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". \n",
            "\n",
            "Generated question:  when did beyonce start becoming popular ? <end> \n",
            "\n",
            "Target question:  When did Beyonce start becoming popular?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "r0B-8OfDIzfw",
        "outputId": "2dba86f9-8bbf-4dc6-e21f-31185fd42a1e"
      },
      "source": [
        "predict_df.iloc[0,1][:146]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "M0-pKM30Ak8Z",
        "outputId": "d00188ab-ce36-457d-862c-1a0f906a9a09"
      },
      "source": [
        "translate(predict_df.iloc[0,0][:189],predict_df.iloc[0,1][:146],encoder1,encoder2,decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'escalation relationships dumbwaiters witon paints paints paints satavahanas satavahanas montevideo luxuries prevailed montevideo prevailed prevailed analogies leto leto leto valens starve complaining repress numbe statistical shots valens vetoes rockerfeller novels novels novels leto novels leto novels leto nassir leto novels leto nassir leto novels leto novels leto nassir leto novels leto novels leto nassir leto synthesized cabildo novels novels leto novels leto '"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adKdcLBbwV5s"
      },
      "source": [
        "1squad = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/train.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "Qi9x1SpNwmJk",
        "outputId": "d2603c50-8327-4ccd-e342-603dcf0ca312"
      },
      "source": [
        "squad.iloc[298,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'On December 13, 2013, Beyoncé unexpectedly released her eponymous fifth studio album on the iTunes Store without any prior announcement or promotion. The album debuted atop the Billboard 200 chart, giving Beyoncé her fifth consecutive number-one album in the US. This made her the first woman in the chart\\'s history to have her first five studio albums debut at number one. Beyoncé received critical acclaim and commercial success, selling one million digital copies worldwide in six days; The New York Times noted the album\\'s unconventional, unexpected release as significant. Musically an electro-R&B album, it concerns darker themes previously unexplored in her work, such as \"bulimia, postnatal depression [and] the fears and insecurities of marriage and motherhood\". The single \"Drunk in Love\", featuring Jay Z, peaked at number two on the Billboard Hot 100 chart. In April 2014, after much speculation in the weeks before, Beyoncé and Jay Z officially announced their On the Run Tour. It served as the couple\\'s first co-headlining stadium tour together. On August 24, 2014, she received the Video Vanguard Award at the 2014 MTV Video Music Awards. Knowles also took home three competitive awards: Best Video with a Social Message and Best Cinematography for \"Pretty Hurts\", as well as best collaboration for \"Drunk in Love\". In November, Forbes reported that Beyoncé was the top-earning woman in music for the second year in a row—earning $115 million in the year, more than double her earnings in 2013. Beyoncé was reissued with new material in three forms: as an extended play, a box set, as well as a full platinum edition.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "WaNcPjR5ww_t",
        "outputId": "2e5be626-87ed-4348-fe05-b5f379409749"
      },
      "source": [
        "squad.iloc[60893,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In 1992, Madonna had a role in A League of Their Own as Mae Mordabito, a baseball player on an all-women\\'s team. She recorded the film\\'s theme song, \"This Used to Be My Playground\", which became a Hot 100 number one hit. The same year, she founded her own entertainment company, Maverick, consisting of a record company (Maverick Records), a film production company (Maverick Films), and associated music publishing, television broadcasting, book publishing and merchandising divisions. The deal was a joint venture with Time Warner and paid Madonna an advance of $60 million. It gave her 20% royalties from the music proceedings, one of the highest rates in the industry, equaled at that time only by Michael Jackson\\'s royalty rate established a year earlier with Sony. The first release from the venture was Madonna\\'s book, titled Sex. It consisted of sexually provocative and explicit images, photographed by Steven Meisel. The book received strong negative reaction from the media and the general public, but sold 1.5 million copies at $50 each in a matter of days. At the same time she released her fifth studio album, Erotica, which debuted at number two on the Billboard 200. Its title track peaked at number three on the Billboard Hot 100. Erotica also produced five singles: \"Deeper and Deeper\", \"Bad Girl\", \"Fever\", \"Rain\" and \"Bye Bye Baby\". Madonna had provocative imagery featured in the erotic thriller, Body of Evidence, a film which contained scenes of sadomasochism and bondage. It was poorly received by critics. She also starred in the film Dangerous Game, which was released straight to video in North America. The New York Times described the film as \"angry and painful, and the pain feels real.\"'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uJL5AQMGw1fo",
        "outputId": "22fba8f0-eb61-4d84-fb6f-8adfdeb49fa5"
      },
      "source": [
        "squad.iloc[60893,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the name of the erotic thriller that shows scenes of sadomasochism and bondage?'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cDQKstVdUNn7",
        "outputId": "dc0268c6-26c2-4a76-ae77-de7b7a93cb77"
      },
      "source": [
        "translate(df[\"given_context\"][0], df[\"reference_context\"][0],encoder1,encoder2,decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what was the name of the film that madonna directed ? <end> '"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dIZgRHgYiZrx",
        "outputId": "66c74120-e927-4832-ad0a-6bb94c857da4"
      },
      "source": [
        "df[\"given_context\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "DohlGMCBibuz",
        "outputId": "6bd73df0-dbab-4a52-ebdb-307221600fdc"
      },
      "source": [
        "df[\"reference_context\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". '"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnxUIivOMfwq"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-3Bcev6bH4Q"
      },
      "source": [
        "newdf = df.drop_duplicates(\n",
        "  subset = ['given_context', 'reference_context'],\n",
        "  keep = 'first').reset_index(drop = True)\n",
        "  \n",
        "# print latest dataframe\n",
        "display(newdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhrgDlMkbH2I"
      },
      "source": [
        "newdf.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vYXrnKxnitH",
        "outputId": "cac82c19-cb34-402a-e7a5-1dafda79a072"
      },
      "source": [
        "# BLEU SCORE\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [['this',\"is\",\"a\",\"good\",\"day\"]]\n",
        "candidate = [['hey',\"hi\",\"bad\"],[\"its\",\"not\"]]\n",
        "score = sentence_bleu(ref, given, weights=(1, 0, 0, 0))\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WiK-LAst6b0"
      },
      "source": [
        "ref = [x.split() for x in grp.iloc[0,1]]\n",
        "given = ref[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGdf4aObuNdz",
        "outputId": "30a6688d-523d-4598-ff02-e5010ded324d"
      },
      "source": [
        "given"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['When', 'did', 'Beyonce', 'start', 'becoming', 'popular?']"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sViiZj2JMxgX"
      },
      "source": [
        "squad = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/train.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDsycQ4Vpkpp"
      },
      "source": [
        "squad = squad[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIglNlz7qJk0"
      },
      "source": [
        "grp = squad.groupby('context',sort=False).agg(new=('question', list)).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1GrVcBt1B8"
      },
      "source": [
        "def create_reference_dataset_bleu(df):\n",
        "  # Takes the SQUAD dataset\n",
        "  # Groups by \"context\"\n",
        "  # For every unique contxet, a list of all the questions in the squad dataset is created\n",
        "  grp = df.groupby('context',sort=False).agg(new=('question', list)).reset_index()\n",
        "  return grp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p7FWRqd0cR4"
      },
      "source": [
        "def create_candidate_dataset_bleu(df):\n",
        "  # Takes the SQUAD dataset\n",
        "  # Groups by \"context\"\n",
        "  # For every unique contxet, a list of all the questions in the squad dataset is created\n",
        "  grp = df.groupby('context',sort=False).agg(new=('question', list)).reset_index()\n",
        "  return grp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw2FAo5JycrB",
        "outputId": "b4affe44-447f-405c-d0b8-2e768e8db95d"
      },
      "source": [
        "# Loading Trained Model\n",
        "attention1 = BahdanauAttention(units)\n",
        "attention2 = BahdanauAttention(units)\n",
        "encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, None, None)\n",
        "encoder1.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder1-6\",)\n",
        "encoder2.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder2-6\")\n",
        "decoder.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/decoder6\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f20f6dec090>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFJt1Gs9xK0R"
      },
      "source": [
        "def calculate_bleu(df):\n",
        "  reference = []\n",
        "  data = create_reference_dataset_bleu(df)\n",
        "  bleu = 0\n",
        "  for i in range(len(data[:2])):\n",
        "    reference = [x.split() for x in grp.iloc[i,1]]\n",
        "    print(len(data))\n",
        "    print(len(df))\n",
        "  return ref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7BuFQ_DzuPs",
        "outputId": "4ab3b396-32ef-4015-9e19-85d2fe02e329"
      },
      "source": [
        "len(create_dataset_bleu(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18877"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyczWcTzx0Ct"
      },
      "source": [
        "calculate_bleu(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "C5DMp0EwsarW",
        "outputId": "0028d2d7-9f3d-43f3-f9f1-c486311216e7"
      },
      "source": [
        "grp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[When did Beyonce start becoming popular?, Wha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>[After her second solo album, what other enter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
              "      <td>[In her music, what are some recurring element...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
              "      <td>[Beyonce's younger sibling also sang with her ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
              "      <td>[What town did Beyonce go to school in?, Who w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context                                                new\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  [When did Beyonce start becoming popular?, Wha...\n",
              "1  Following the disbandment of Destiny's Child i...  [After her second solo album, what other enter...\n",
              "2  A self-described \"modern-day feminist\", Beyonc...  [In her music, what are some recurring element...\n",
              "3  Beyoncé Giselle Knowles was born in Houston, T...  [Beyonce's younger sibling also sang with her ...\n",
              "4  Beyoncé attended St. Mary's Elementary School ...  [What town did Beyonce go to school in?, Who w..."
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "5SgkyYG9o0Uv",
        "outputId": "7ae76ce8-cde7-4cc5-91a0-2866bab2623c"
      },
      "source": [
        "squad.groupby('context').apply(f).iloc[2][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Beyonce's younger sibling also sang with her in what band?Where did Beyonce get her name from?What race was Beyonce's father?Beyonce's childhood home believed in what religion?Beyonce's father worked as a sales manager for what company?Beyonce's mother worked in what industry?What younger sister of Beyonce also appeared in Destiny's Child?Beyonce is a descendent of what Arcadian leader?What company did Beyoncé's father work for when she was a child?What did Beyoncé's mother own when Beyoncé was a child?What is the name of Beyoncé's younger sister?Beyoncé is a descendant of which Acadian leader?Beyoncé was raised in what religion?\""
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTah5G9KtimQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5SSCuX0KOw"
      },
      "source": [
        "### Doing it for a simple example batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjGlZkRj0NuC"
      },
      "source": [
        "# attention = BahdanauAttention(units)\n",
        "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# decoder = DecoderWithAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention)\n",
        "# loss = 0\n",
        "# with tf.GradientTape() as tape: # for automatic differentiation\n",
        "#     sample_hidden = encoder.initialize_hidden_state()\n",
        "#     enc_output, enc_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "#     dec_hidden = enc_hidden\n",
        "#     dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "#     # Teacher forcing - feeding the target as the next input\n",
        "#     for t in range(1, example_target_batch.shape[1]):\n",
        "#         # passing enc_output to the decoder\n",
        "#         predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "#         loss += loss_function(example_target_batch[:, t], predictions)\n",
        "#         # using teacher forcing\n",
        "#         dec_input = tf.expand_dims(example_target_batch[:, t], 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
