{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PreLSTMEncoder_based_Seq2seq_with_dual_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfbpjs2fgzuT",
        "outputId": "327eb831-2cb3-459b-e3f1-45bc5739d0a7"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTBFuHj0u3sC"
      },
      "source": [
        "# SQUAD DEV SET ANALYSIS "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjF9hLAFu7Jr"
      },
      "source": [
        "dev = pd.read_csv(\"/content/drive/MyDrive/AutomatedQuestionGeneration/Topic_Bifurcated_dev.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaebmHR6u7Hj"
      },
      "source": [
        "dev.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "-y1Cakuyvk04",
        "outputId": "f7af5e13-5e0c-42d8-c4de-0d25a12446e6"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>249</td>\n",
              "      <td>Carolina Panthers</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>403</td>\n",
              "      <td>Santa Clara, California</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>355</td>\n",
              "      <td>Levi's Stadium</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "3  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "6  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "7  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Rm5SrOvwRD"
      },
      "source": [
        "dev = dev.drop_duplicates(subset = ['context'],keep = 'first').reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "R2398rpzv6pS",
        "outputId": "b243f0c1-f905-430b-9d48-2132d586ba73"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>The Panthers finished the regular season with ...</td>\n",
              "      <td>Which Carolina Panthers player was named Most ...</td>\n",
              "      <td>77</td>\n",
              "      <td>Cam Newton</td>\n",
              "      <td>The Broncos finished the regular season with a...</td>\n",
              "      <td>The Panthers finished the regular season with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>The Broncos took an early lead in Super Bowl 5...</td>\n",
              "      <td>Who was the Super Bowl 50 MVP?</td>\n",
              "      <td>248</td>\n",
              "      <td>Von Miller</td>\n",
              "      <td>The Broncos took an early lead in Super Bowl 5...</td>\n",
              "      <td>Newton was limited by Denver's defense, which ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>CBS broadcast Super Bowl 50 in the U.S., and c...</td>\n",
              "      <td>Which network broadcasted Super Bowl 50 in the...</td>\n",
              "      <td>0</td>\n",
              "      <td>CBS</td>\n",
              "      <td>The Super Bowl 50 halftime show was headlined ...</td>\n",
              "      <td>CBS broadcast Super Bowl 50 in the U.S., and c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>In early 2012, NFL Commissioner Roger Goodell ...</td>\n",
              "      <td>Who was the NFL Commissioner in early 2012?</td>\n",
              "      <td>32</td>\n",
              "      <td>Roger Goodell</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The Panthers finished the regular season with ...\n",
              "2  Super_Bowl_50  ...  Newton was limited by Denver's defense, which ...\n",
              "3  Super_Bowl_50  ...  CBS broadcast Super Bowl 50 in the U.S., and c...\n",
              "4  Super_Bowl_50  ...                                                NaN\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Lr8kejxxvj",
        "outputId": "5a77051e-a4cd-48cb-80b6-f7d337b3e269"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "3IeB_gDadfhr",
        "outputId": "1185ca23-c965-421c-8c04-b7accf3004d4"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/AutomatedQuestionGeneration/Topic_Bifurcated_SQUAD2.csv\")\n",
        "df = df.drop_duplicates(subset = ['given_context', 'reference_context'],keep = 'first').reset_index(drop = True)\n",
        "# df = df[:100]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be86cf3aeaaa14008c9076</td>\n",
              "      <td>After her second solo album, what other entert...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>acting</td>\n",
              "      <td>207.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6e823aeaaa14008c962a</td>\n",
              "      <td>Which album was darker in tone from her previo...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ...                                  reference_context\n",
              "0  56be85543aeaaa14008c9063  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1  56be85543aeaaa14008c9065  ...  Born and raised in Houston, Texas, she perform...\n",
              "2  56bf6b0f3aeaaa14008c9605  ...  Managed by her father, Mathew Knowles, the gro...\n",
              "3  56be86cf3aeaaa14008c9076  ...  Beyoncé also ventured into acting, with a Gold...\n",
              "4  56bf6e823aeaaa14008c962a  ...  Following the disbandment of Destiny's Child i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOsMShvlg4ua"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load data set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeHAlqRBeaPJ"
      },
      "source": [
        "# Train Set\n",
        "reference_context = df[\"reference_context\"].apply(lambda x: x).tolist()\n",
        "given_context = df[\"given_context\"].apply(lambda x: x).tolist()\n",
        "target_ques = df[\"question\"].apply(lambda x: x).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8SYzSr-xFiT"
      },
      "source": [
        "# Test Set\n",
        "reference_context = dev[\"reference_context\"].apply(lambda x: x).tolist()\n",
        "given_context = dev[\"given_context\"].apply(lambda x: x).tolist()\n",
        "target_ques = dev[\"question\"].apply(lambda x: x).tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDa3tH1hRyNq"
      },
      "source": [
        "*   Clean the sentences by removing special characters.\n",
        "*   Add a start and end token to each sentence.\n",
        "*   Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "*   Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKjEWl6WhZyk"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(str(w).lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\",\"¿\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  # remove extra space\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc58-K0XhdCM",
        "outputId": "f4e94ab9-7ea1-4029-b5c2-abe45ced2621"
      },
      "source": [
        "en_sentence = u\"Beyonce's last record was called Lemonade\"\n",
        "sp_sentence = u\"Beyonce was born in 1950\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode(\"UTF-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> beyonce s last record was called lemonade <end>\n",
            "<start> beyonce was born in <end>\n",
            "b'<start> beyonce was born in <end>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYdaxYI2df3H",
        "outputId": "b10fe8fc-4a31-4319-da99-9d3134f0657d"
      },
      "source": [
        "reference_context[:1], given_context[:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". '],\n",
              " [\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \"])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hw9ct4OhsRA",
        "outputId": "bb6fb0ab-1330-4a40-8267-f6680f467007"
      },
      "source": [
        "def create_dataset_squad(context_param, given_param, target_ques):\n",
        "  reference_context = []\n",
        "  given_context = []\n",
        "  question = []\n",
        "  for c,g,q in zip(context_param,given_param, target_ques):\n",
        "    reference_context.append(preprocess_sentence(str(c)))\n",
        "    given_context.append(preprocess_sentence(str(g)))\n",
        "    question.append(preprocess_sentence(q))\n",
        "  return tuple(reference_context), tuple(given_context), tuple(question)\n",
        "\n",
        "r, g, q = create_dataset_squad(reference_context, given_context, target_ques)\n",
        "print(r[-1])\n",
        "print(g[-1])\n",
        "print(q[-1])\n",
        "print(len(r), len(q))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this activity has been further enhanced by establishing formal relationships with other cities motsumoto city of japan , rochester of the usa , yangon formerly rangoon of myanmar , xi an of the people s republic of china , minsk of belarus , and pyongyang of the democratic republic of korea . kmc s constant endeavor is to enhance its interaction with saarc countries , other international agencies and many other major cities of the world to achieve better urban management and developmental programs for kathmandu . <end>\n",
            "<start> kathmandu metropolitan city kmc , in order to promote international relations has established an international relations secretariat irc . kmc s first international relationship was established in with the city of eugene , oregon , united states . <end>\n",
            "<start> what was yangon previously known as ? <end>\n",
            "43275 43275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCAYRuTlh5DY"
      },
      "source": [
        "# Tokenize the sentence into list of words(integers) and pad the sequence to the same length\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM87LBJ4ewey"
      },
      "source": [
        "def load_dataset(reference, given, target):\n",
        "  # creating cleaned input, output pairs\n",
        "  #targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  reference_tensor, reference_tokenizer = tokenize(reference)\n",
        "  given_tensor, given_tokenizer = tokenize(given)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(target)\n",
        "\n",
        "  return reference_tensor, given_tensor, target_tensor, reference_tokenizer, given_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EasB_FLig5c",
        "outputId": "9054f232-045d-410b-865b-7fdc629a04c5"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "#num_examples = 30000\n",
        "# input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(c,q)\n",
        "reference_tensor, given_tensor, target_tensor, reference_tokenizer, given_tokenizer, targ_tokenizer = load_dataset(r, g, q)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_reference, max_length_given = target_tensor.shape[1], reference_tensor.shape[1], given_tensor.shape[1]\n",
        "print(max_length_targ, max_length_reference, max_length_given)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62 452 414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMgiyh8igJ-3"
      },
      "source": [
        "# train = zip(reference_tensor, given_tensor)\n",
        "# train = pd.DataFrame(data=train, index=None)\n",
        "# train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oS6mqluirWG"
      },
      "source": [
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(train, target_tensor, test_size=0.2)\n",
        "reference_tensor_train, reference_tensor_val, given_tensor_train, given_tensor_val, target_tensor_train, target_tensor_val = train_test_split(reference_tensor, given_tensor, target_tensor,random_state = 101)\n",
        "\n",
        "# Show length\n",
        "# print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "# print(input_tensor_train[1])\n",
        "# print()\n",
        "# print(target_tensor_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKWru44VgqfN"
      },
      "source": [
        "# input_tensor_train\n",
        "# reference_tensor_train = input_tensor_train[0].values\n",
        "# given_tensor_train = input_tensor_train[1].values\n",
        "# \n",
        "# reference_tensor_val = input_tensor_val[0].values\n",
        "# given_tensor_val = input_tensor_val[1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK3NE1vUityv",
        "outputId": "02e036a1-6f49-4018-d030-17fb8a5e69dd"
      },
      "source": [
        "# Configuration \n",
        "BUFFER_SIZE = len(reference_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(reference_tensor_train)//BATCH_SIZE\n",
        "steps_per_epoch_val = len(reference_tensor_val)//BATCH_SIZE\n",
        "embedding_dim = 256  # for word embedding\n",
        "units = 1024  # dimensionality of the output space of RNN\n",
        "vocab_inp_size = len(given_tokenizer.word_index) + len(reference_tokenizer.word_index) + 1 \n",
        "\n",
        "vocab_tar_size = len(targ_tokenizer.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((reference_tensor_train, given_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((reference_tensor_val, given_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_reference_batch, example_given_batch, example_target_batch = next(iter(dataset))\n",
        "example_reference_batch.shape, example_given_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 452]), TensorShape([16, 414]), TensorShape([16, 62]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzo3ZvYpv4KE"
      },
      "source": [
        "# Seq2seq model: Two Encoders and DecoderWithDualAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFjq9Ta3wA8F"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.LSTM = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,  # Whether to return the last output in the output sequence, or the full sequence. \n",
        "                                   return_state=True,  # Whether to return the last state in addition to the output.\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(Encoder, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'batch_sz': self.batch_sz,\n",
        "            'enc_units': self.enc_units,\n",
        "            # 'embedding': self.embedding,\n",
        "            # 'LSTM': self.LSTM,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embedding_dim': self.embedding_dim\n",
        "        })\n",
        "        return config \n",
        "\n",
        "  \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, hidden_h, hidden_c = self.LSTM(x, initial_state = hidden)\n",
        "    return output, [hidden_h,hidden_c]\n",
        "\n",
        "  @tf.function\n",
        "  def initialize_hidden_state(self):\n",
        "    init_state = [tf.zeros([self.batch_sz, self.enc_units]) for i in range(2)]\n",
        "    return init_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4GoRSHSwScu",
        "outputId": "38270c43-06c3-4b8d-e9ee-f78463674d49"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "#sample_hidden = [tf.zeros([1,1024]) for i in range(2)]\n",
        "sample_output, sample_hidden= encoder(example_reference_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
        "print ('Encoder Carry state shape: (batch size, units) {}'.format(sample_hidden[1].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (16, 452, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (16, 1024)\n",
            "Encoder Carry state shape: (batch size, units) (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSWViCN635g8"
      },
      "source": [
        "# Additive attention\n",
        "\n",
        "![alt text](https://i.ibb.co/BqDYNP1/additive.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXoFjEEoVPlI"
      },
      "source": [
        "#tf.keras.layers.Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOzyie8DLjEI"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    self.units = units\n",
        "    super(BahdanauAttention, self).__init__(**kwargs)\n",
        "\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(BahdanauAttention, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'units': self.units,   \n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  \n",
        "  \n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "    # print(\"Query : ,\",query)\n",
        "    \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    # print(\"Query with time axis: ,\",query_with_time_axis.shape)\n",
        "    # query_with_time_axis = query\n",
        "    # values = query_with_time_axis\n",
        "    # print(\"Values: ,\",values.shape)\n",
        "    # print(values.shape)\n",
        "    # print(query_with_time_axis.shape)\n",
        "\n",
        "    # (64, 358, 1024) ->values.shape\n",
        "    # (64, 1, 1024) -> query_with_time_axis.shape\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(query_with_time_axis)))\n",
        "    # print(\"W1(Values): \", self.W1(values).shape)\n",
        "    # print(\"W2(Query_with_time_axis): \", self.W2(query_with_time_axis).shape)\n",
        "    # print(\"Score: \", score.shape)\n",
        "    # (64, 358, 1024) -> self.W1(values).shape\n",
        "    # (64, 1, 1024) -> self.W2(query_with_time_axis).shape\n",
        "    # (64, 358, 1) -> score.shape\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    # print(context_vector.shape)\n",
        "    # (64, 358, 1024)\n",
        "\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    # print(context_vector.shape)\n",
        "    # (64, 1024)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNsmnspoSbz8"
      },
      "source": [
        "class DecoderWithDualAttention(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_layer1 = None, attention_layer2 = None):\n",
        "    super(DecoderWithDualAttention, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.LSTM = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # used for attention\n",
        "    self.attention1 = attention_layer1\n",
        "    self.attention2 = attention_layer2\n",
        "\n",
        "  \n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super(DecoderWithDualAttention, self).get_config().copy()\n",
        "        #config = {}\n",
        "        config.update({\n",
        "            'batch_sz': self.batch_sz,\n",
        "            'dec_units': self.dec_units,\n",
        "            # 'embedding': self.embedding,\n",
        "            # 'LSTM': self.LSTM,\n",
        "            # 'fc' : self.fc,\n",
        "            'attention_layer1':self.attention1,\n",
        "            'attention_layer2':self.attention2,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embedding_dim': self.embedding_dim\n",
        "        })\n",
        "        return config\n",
        "    \n",
        "  \n",
        "  \n",
        "  \n",
        "  def call(self, x, hidden1, hidden2, enc_output1, enc_output2):\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    y = x\n",
        "    attention_weights1 = None\n",
        "    \n",
        "    # Attention 1 is used to associate the target word embedding with the reference context embedding (attended)\n",
        "    if self.attention1:\n",
        "      # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "      context_vector1, attention_weights1 = self.attention1(hidden1[0], enc_output1)\n",
        "      # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "      x = tf.concat([tf.expand_dims(context_vector1, 1), x], axis=-1)\n",
        "      # x = tf.concat([context_vector1, x], axis=-1)\n",
        "          \n",
        "    attention_weights2 = None\n",
        "    \n",
        "    # Attention 2 is used to associate the target word embedding with the given context embedding (attended)\n",
        "    # if self.attention2:\n",
        "    #   # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    #   context_vector2, attention_weights2 = self.attention2(hidden2[0], enc_output2)\n",
        "    #   # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    #   y = tf.concat([tf.expand_dims(context_vector2, 1), y], axis=-1)\n",
        "    #   # y = tf.concat([context_vector2, y], axis=-1)\n",
        "  \n",
        "\n",
        "    # passing the concatenated vector to the LSTM\n",
        "    output1, state_h1, state_c1 = self.LSTM(x, initial_state = hidden1)\n",
        "    output2, state_h2, state_c2= self.LSTM(y, initial_state = hidden2)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output1 = tf.reshape(output1, (-1, output1.shape[2]))\n",
        "    output2 = tf.reshape(output2, (-1, output2.shape[2]))\n",
        "\n",
        "    output = tf.concat([output1, output2], axis = 1)\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    final_output = self.fc(output)\n",
        "\n",
        "    return final_output, [state_h1,state_c1], [state_h2,state_c2], attention_weights1, attention_weights2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oozBiV1Khj"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFLLiTBgnwG-",
        "outputId": "7040502f-8bc9-41e6-debd-5dc565a2cc41"
      },
      "source": [
        "print(loss_object([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))\n",
        "print(loss_function([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1.063386  1.3633859], shape=(2,), dtype=float32)\n",
            "tf.Tensor(1.2133859, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz6c3l931TC8"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def get_train_step_func():\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape: # for automatic differentiation\n",
        "      enc_output1, enc_hidden1 = encoder1(inp, enc_hidden1)\n",
        "      enc_output2, enc_hidden2 = encoder2(aux, enc_hidden2)\n",
        "\n",
        "      # Initialising decoder hidden states with the corresponding hidden states of the encoder (one for given context encoder and the other for reference context encoder)\n",
        "      dec_hidden1 = enc_hidden1\n",
        "      dec_hidden2 = enc_hidden2\n",
        "      # enc_hidden -> \n",
        "\n",
        "      # Check whether we should concatenate dec_hidden and dec_hidden2 or keep it separate\n",
        "      # dec_hidden2 = enc_hidden2\n",
        "\n",
        "      dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      # Teacher forcing - feeding the target as the next input\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        # print(\"Iteration: \",t)\n",
        "        # passing enc_output to the decoder\n",
        "        # print(\"EncOutput1 before: \",enc_output1)\n",
        "        predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, dec_hidden2, enc_output1, enc_output2)\n",
        "        # print(\"EncOutput1 after: \",enc_output1)\n",
        "        \n",
        "        # x, y, hidden1, hidden2, enc_output1, enc_output2\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder1.trainable_variables + encoder2.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "    \n",
        "  return train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zym-buZ_TEg"
      },
      "source": [
        "def caculate_validation_loss(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder):\n",
        "  loss = 0\n",
        "  enc_output1, enc_hidden1 = encoder1(inp, enc_hidden1)\n",
        "  enc_output2, enc_hidden2 = encoder2(aux, enc_hidden1)\n",
        "  dec_hidden1 = enc_hidden1\n",
        "  dec_hidden2 = enc_hidden2\n",
        "  dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "  # Teacher forcing - feeding the target as the next input\n",
        "  # for t in range(1, targ.shape[1]):\n",
        "  #   predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "  #   loss += loss_function(targ[:, t], predictions)\n",
        "  #   dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # loss = loss / int(targ.shape[1])\n",
        "  # return loss\n",
        "\n",
        "  for t in range(1, targ.shape[1]):\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, dec_hidden2, enc_output1, enc_output2)\n",
        "        # x, y, hidden1, hidden2, enc_output1, enc_output2\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  loss = loss / int(targ.shape[1])\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMo0_PVaJiP7"
      },
      "source": [
        " def training_seq2seq(epochs, attention1, attention2):\n",
        "  encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)\n",
        "  train_step_func = get_train_step_func()\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print()\n",
        "    print(epoch)\n",
        "    start = time.time()\n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (aux, inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      print(batch)\n",
        "      batch_loss = train_step_func(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      # inp, targ, aux, enc_hidden, enc_hidden2, encoder, encoder2, decoder\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "        \n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_val_loss = 0\n",
        "    for (batch, (aux, inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch)):\n",
        "      val_loss = caculate_validation_loss(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      total_val_loss += val_loss\n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder1, encoder2, decoder, training_loss, validation_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn4N30XinpVY"
      },
      "source": [
        "# Training Starts Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPmQAjf9ULa"
      },
      "source": [
        "def resume_training_seq2seq(epochs, attention1, attention2):\n",
        "  encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)\n",
        "  encoder1.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder1-10\")\n",
        "  encoder2.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder2-10\")\n",
        "  decoder.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/decoder10\")\n",
        "  print(\"\\n\\nLoading done\\n\\n\")\n",
        "  train_step_func = get_train_step_func()\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print()\n",
        "    print(epoch)\n",
        "    start = time.time()\n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (aux, inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      print(batch)\n",
        "      batch_loss = train_step_func(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      # inp, targ, aux, enc_hidden, enc_hidden2, encoder, encoder2, decoder\n",
        "      total_loss += batch_loss\n",
        "      print(\"\\n\\nBatch done\\n\\n\")\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "        \n",
        "    enc_hidden1 = encoder1.initialize_hidden_state()\n",
        "    enc_hidden2 = encoder2.initialize_hidden_state()\n",
        "    total_val_loss = 0\n",
        "    for (batch, (aux, inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch)):\n",
        "      val_loss = caculate_validation_loss(inp, targ, aux, enc_hidden1, enc_hidden2, encoder1, encoder2, decoder)\n",
        "      total_val_loss += val_loss\n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                        training_loss[-1], validation_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  return encoder1, encoder2, decoder, training_loss, validation_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKjqurFVY0u"
      },
      "source": [
        "## Training seq2seq with Bahdanau attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUKivtyYix6"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "attention1 = BahdanauAttention(units)\n",
        "attention2 = BahdanauAttention(units)\n",
        "print(\"Running seq2seq model with Bahdanau attention\")\n",
        "encoder1_bah, encoder2_bah2, decoder_bah, training_loss, validation_loss = training_seq2seq(epochs, attention1, attention2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eev4IrPIZj-S"
      },
      "source": [
        "tf.keras.models.save_model(encoder1_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/encoder1-final\")\n",
        "tf.keras.models.save_model(encoder2_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/encoder2-final\")\n",
        "tf.keras.models.save_model(decoder_bah,\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/TF2.6saved/lstm_output_dual/decoder-final\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veo1kmoOCNWD"
      },
      "source": [
        "# Loading Trained Model\n",
        "attention1 = BahdanauAttention(units)\n",
        "attention2 = BahdanauAttention(units)\n",
        "encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention1, attention2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlX_NB9fQlEY",
        "outputId": "bab43d5b-8e9c-40ad-c72d-09870a37491b"
      },
      "source": [
        "encoder1 = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/encoder1-final/\")\n",
        "encoder2 = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/encoder2-final\")\n",
        "decoder = tf.keras.models.load_model(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/decoder-final/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9Fn5HrOO6i"
      },
      "source": [
        "def translate(g_context, r_context, encoder1, encoder2, decoder):\n",
        "  attention_plot1 = np.zeros((max_length_targ, max_length_given))\n",
        "  attention_plot2 = np.zeros((max_length_targ, max_length_reference))\n",
        "\n",
        "  g_context = preprocess_sentence(g_context)\n",
        "\n",
        "  given = [given_tokenizer.word_index[i] for i in g_context.split(' ')]\n",
        "  given = tf.keras.preprocessing.sequence.pad_sequences([given],\n",
        "                                                         maxlen=max_length_given,\n",
        "                                                         padding='post')\n",
        "  # given = tf.convert_to_tensor(given)\n",
        "  given = tf.reshape(tf.convert_to_tensor(given),shape = [-1,max_length_given])\n",
        "\n",
        "  \n",
        "  r_context = preprocess_sentence(r_context)\n",
        "  \n",
        "  ref = [reference_tokenizer.word_index[i] for i in r_context.split(' ')]\n",
        "  ref = tf.keras.preprocessing.sequence.pad_sequences([ref],\n",
        "                                                         maxlen=max_length_reference,\n",
        "                                                         padding='post')\n",
        "  # ref = tf.convert_to_tensor(ref)\n",
        "  ref = tf.reshape(tf.convert_to_tensor(ref),shape = [-1,max_length_reference])\n",
        "\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units)) for i in range(2)]\n",
        "  # hidden = encoder1.initialize_hidden_state()\n",
        "  enc_out1, enc_hidden1 = encoder1(given, hidden)\n",
        "  enc_out2, enc_hidden2 = encoder2(ref, hidden)\n",
        "\n",
        "  dec_hidden1 = enc_hidden1\n",
        "  dec_hidden2 = enc_hidden2\n",
        "  dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    # predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "    #                                                      dec_hidden,\n",
        "    #                                                      enc_out)\n",
        "\n",
        "    predictions, dec_hidden1, dec_hidden2, _, _ = decoder(dec_input, dec_hidden1, dec_hidden2, enc_out1, enc_out2)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy() \n",
        "    # print(predicted_id)\n",
        "\n",
        "    result += targ_tokenizer.index_word[predicted_id] + ' '\n",
        "    # print(result)\n",
        "\n",
        "    # until the predicted word is <end>.\n",
        "    if targ_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result\n",
        "\n",
        "    # the predicted ID is fed back into the model, no teacher forcing.\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "hkePMDa2weMS",
        "outputId": "9228222b-5a7a-4016-f91e-f01879941d0b"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>249</td>\n",
              "      <td>Carolina Panthers</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>403</td>\n",
              "      <td>Santa Clara, California</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>355</td>\n",
              "      <td>Levi's Stadium</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "3  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "6  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "7  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XOdCTw9Izk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a943d065-111a-467f-ae49-a02e3a53cac5"
      },
      "source": [
        "translate(dev.iloc[10,5],dev.iloc[10,6], encoder1, encoder2, decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what was the first year that the cia s budget was disclosed ? <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2M_D13_T3mrD",
        "outputId": "e2a36c67-6fe5-4712-e850-3dcfdd68a02b"
      },
      "source": [
        "dev.iloc[10,5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. \""
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "d5GnhZZ63oga",
        "outputId": "c145f80d-88ec-4b9d-ff4d-8e05d3c77023"
      },
      "source": [
        "dev.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50. '"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qfwxeCpo9JkB",
        "outputId": "b33fa887-229c-44b8-cdd2-cb4dc029653f"
      },
      "source": [
        "dev.iloc[10,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the theme of Super Bowl 50?'"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "SZqH2rnW3qwc",
        "outputId": "d6348c4d-0da2-4fb1-ebe8-b2bd1857ba01"
      },
      "source": [
        "dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_index</th>\n",
              "      <th>answer</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>177</td>\n",
              "      <td>Denver Broncos</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>249</td>\n",
              "      <td>Carolina Panthers</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The American Football Conference (AFC) champio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>403</td>\n",
              "      <td>Santa Clara, California</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Super_Bowl_50</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>355</td>\n",
              "      <td>Levi's Stadium</td>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>The game was played on February 7, 2016, at Le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           title  ...                                  reference_context\n",
              "0  Super_Bowl_50  ...  Super Bowl 50 was an American football game to...\n",
              "1  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "3  Super_Bowl_50  ...  The American Football Conference (AFC) champio...\n",
              "6  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "7  Super_Bowl_50  ...  The game was played on February 7, 2016, at Le...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d2QW9XnpIWW",
        "outputId": "7e94690b-a8fe-49c0-c6b2-fdfb3a87278e"
      },
      "source": [
        "try:\n",
        "  translate(df.iloc[8578,6],df.iloc[8578,7], encoder1, encoder2, decoder)\n",
        "except:\n",
        "  print(\"Error\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Im3qC71U2e7a",
        "outputId": "8898a46f-7ee0-49d2-cd49-23ff582ba7f5"
      },
      "source": [
        "df.iloc[10000,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. \""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0r37uLAm2hyI",
        "outputId": "1d6275c7-55b8-4f4d-d5ac-ed79ba8a2a4a"
      },
      "source": [
        "df.iloc[10000,7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman. '"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mBlYXuZv6kvM",
        "outputId": "f7db1746-04b8-4024-bb13-8dc617bfa72e"
      },
      "source": [
        "df.iloc[10100,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Postmarital residence among hunter-gatherers tends to be matrilocal, at least initially. Young mothers can enjoy childcare support from their own mothers, who continue living nearby in the same camp. '"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nN8_InQ4D3k"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzXHiKQhNkRS"
      },
      "source": [
        "predict_df = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/LSTMpredictions17861.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mThIgZsa3Wew",
        "outputId": "5d10f29b-f54e-4b4b-d8e2-2a388f0060e2"
      },
      "source": [
        "predict_df = df[[\"given_context\",\"reference_context\"]]\n",
        "predict_df.columns = [\"given_context\",\"reference_context\"]\n",
        "predict_df[\"generated_question\"] = 'a'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkmSbsPH4mwh",
        "outputId": "2b5daae1-645d-48dd-a890-19fa59bbe5f6"
      },
      "source": [
        "for i in range(8570,8580):\n",
        "  try:\n",
        "    predict_df[\"generated_question\"][i] = translate(predict_df[\"given_context\"][i], predict_df[\"reference_context\"][i],encoder1,encoder2,decoder)\n",
        "  except:\n",
        "    predict_df[\"given_context\"][i], predict_df[\"reference_context\"][i] = np.nan,np.nan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AWU4goD-I5u"
      },
      "source": [
        "predict_df[8570:8581].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "qmkJ1vvo8o1Y",
        "outputId": "813c8e75-fe41-432a-e4fc-151a83b8be2b"
      },
      "source": [
        "predict_df[8570:8581]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "      <th>generated_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8570</th>\n",
              "      <td>According to Y chromosome studies by Sanchez e...</td>\n",
              "      <td>(2004, 2007), the Somalis are paternally close...</td>\n",
              "      <td>what is the most common surname in the italian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8571</th>\n",
              "      <td>(2005), Cruciani et al. Sanchez et al. Accordi...</td>\n",
              "      <td>According to Y chromosome studies by Sanchez e...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8572</th>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8573</th>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>This mitochondrial clade is common among Ethio...</td>\n",
              "      <td>what is the opposite of blackness ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8574</th>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>what is the most efficient means of transporta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8575</th>\n",
              "      <td>(2006), a significant proportion of the matern...</td>\n",
              "      <td>According to mtDNA studies by Holden (2005) an...</td>\n",
              "      <td>what is the most common soundboard shape ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8576</th>\n",
              "      <td>According to an autosomal DNA study by Hodgson...</td>\n",
              "      <td>(2014), the Afro-Asiatic languages were likely...</td>\n",
              "      <td>what was the landline growth rate of somalia i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8577</th>\n",
              "      <td>According to an autosomal DNA study by Hodgson...</td>\n",
              "      <td>(2014), the Afro-Asiatic languages were likely...</td>\n",
              "      <td>what is the posulated homeland region of the r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8578</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8579</th>\n",
              "      <td>The history of Islam in Somalia is as old as t...</td>\n",
              "      <td>The early persecuted Muslims fled to various p...</td>\n",
              "      <td>what is the name of the nomad tribes that hunt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8580</th>\n",
              "      <td>The early persecuted Muslims fled to various p...</td>\n",
              "      <td>The history of Islam in Somalia is as old as t...</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          given_context  ...                                 generated_question\n",
              "8570  According to Y chromosome studies by Sanchez e...  ...  what is the most common surname in the italian...\n",
              "8571  (2005), Cruciani et al. Sanchez et al. Accordi...  ...  what is the most efficient means of transporta...\n",
              "8572  According to mtDNA studies by Holden (2005) an...  ...  what is the most efficient means of transporta...\n",
              "8573  According to mtDNA studies by Holden (2005) an...  ...         what is the opposite of blackness ? <end> \n",
              "8574  (2006), a significant proportion of the matern...  ...  what is the most efficient means of transporta...\n",
              "8575  (2006), a significant proportion of the matern...  ...  what is the most common soundboard shape ? <end> \n",
              "8576  According to an autosomal DNA study by Hodgson...  ...  what was the landline growth rate of somalia i...\n",
              "8577  According to an autosomal DNA study by Hodgson...  ...  what is the posulated homeland region of the r...\n",
              "8578                                                NaN  ...                                                  a\n",
              "8579  The history of Islam in Somalia is as old as t...  ...  what is the name of the nomad tribes that hunt...\n",
              "8580  The early persecuted Muslims fled to various p...  ...                                                  a\n",
              "\n",
              "[11 rows x 3 columns]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jrc-XOtkH_6P",
        "outputId": "6bb0eb4b-8697-4370-be02-d0eb6eae7bb6"
      },
      "source": [
        "#Given_Context\n",
        "df.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PmPeKfbsH_4A",
        "outputId": "25013eba-8d77-4366-e882-d81c9d17484b"
      },
      "source": [
        "#Reference_Context\n",
        "df.iloc[10,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-0zNw7T3FmXm",
        "outputId": "5f716030-2a62-4241-8af5-5b1e9116bdf6"
      },
      "source": [
        "predict_df.iloc[10,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'when did beyonce start becoming a golden globe ? <end> '"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CYjfp027F9vZ",
        "outputId": "080f3043-e0d6-467d-e775-b85b884ca048"
      },
      "source": [
        "df.iloc[10,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the first album Beyoncé released as a solo artist?'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRsovWZukUlm",
        "outputId": "421ae3aa-54d7-49af-88f8-64237399d2fb"
      },
      "source": [
        "import math\n",
        "math.isnan(df[\"reference_context\"][87])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Pa6vIklC4l"
      },
      "source": [
        "pd.isnull(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gpnDtMIt2QOL",
        "outputId": "a1aaf2d6-2760-4b69-bb59-37d8ca22caa3"
      },
      "source": [
        "squad.iloc[8578,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'After August 2008 sites had to be listed on the Open Directory in order to be included. According to Jeff Kaplan of the Internet Archive in November 2010, other sites were still being archived, but more recent captures would become visible only after the next major indexing, an infrequent operation.'"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2jQN-LOZkMLr",
        "outputId": "bec89535-41f0-4265-8171-cd39cb537f76"
      },
      "source": [
        "df.iloc[8578,6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'According to Mohamoud et al. '"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cvg2b1fh0V4r",
        "outputId": "930833ea-d79b-46b9-fb5a-3753eb214000"
      },
      "source": [
        "df.iloc[8578,7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(2006): '"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "5ISyfcYxYlg-",
        "outputId": "ace6c454-8164-4065-8a06-7777b5592989"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "      <th>given_context</th>\n",
              "      <th>reference_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
              "      <td>Who managed the Destiny's Child group?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Mathew Knowles</td>\n",
              "      <td>360.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Managed by her father, Mathew Knowles, the gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56be86cf3aeaaa14008c9076</td>\n",
              "      <td>After her second solo album, what other entert...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>acting</td>\n",
              "      <td>207.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6e823aeaaa14008c962a</td>\n",
              "      <td>Which album was darker in tone from her previo...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Beyoncé also ventured into acting, with a Gold...</td>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ...                                  reference_context\n",
              "0  56be85543aeaaa14008c9063  ...  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...\n",
              "1  56be85543aeaaa14008c9065  ...  Born and raised in Houston, Texas, she perform...\n",
              "2  56bf6b0f3aeaaa14008c9605  ...  Managed by her father, Mathew Knowles, the gro...\n",
              "3  56be86cf3aeaaa14008c9076  ...  Beyoncé also ventured into acting, with a Gold...\n",
              "4  56bf6e823aeaaa14008c962a  ...  Following the disbandment of Destiny's Child i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0EhKrkbWXhJ"
      },
      "source": [
        "newdf = df.drop_duplicates(\n",
        "  subset = ['context'],\n",
        "  keep = 'first').reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQsgNsz2Wu8G"
      },
      "source": [
        "newdf.to_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/UniqueContextBifurcated.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cv59HGDF0i"
      },
      "source": [
        "predict_df.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbR-CA1WIscZ",
        "outputId": "8be77c73-8374-45d4-dcf3-4ba34c2bd881"
      },
      "source": [
        "index = int(input(\"Enter record index: \"))\n",
        "#Given_Context\n",
        "print(\"Given context:\\n\",predict_df.iloc[index,0])\n",
        "#Reference_Context\n",
        "print(\"\\nReference context:\\n\",predict_df.iloc[index,1])\n",
        "# Generated Question\n",
        "print(\"\\nGenerated question: \",predict_df.iloc[index,3])\n",
        "# Target Question\n",
        "print(\"\\nTarget question: \",predict_df.iloc[index,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter record index: 0\n",
            "Given context:\n",
            " Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \n",
            "\n",
            "Reference context:\n",
            " Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". \n",
            "\n",
            "Generated question:  when did beyonce start becoming popular ? <end> \n",
            "\n",
            "Target question:  When did Beyonce start becoming popular?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "r0B-8OfDIzfw",
        "outputId": "2dba86f9-8bbf-4dc6-e21f-31185fd42a1e"
      },
      "source": [
        "predict_df.iloc[0,1][:146]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "M0-pKM30Ak8Z",
        "outputId": "d00188ab-ce36-457d-862c-1a0f906a9a09"
      },
      "source": [
        "translate(predict_df.iloc[0,0][:189],predict_df.iloc[0,1][:146],encoder1,encoder2,decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'escalation relationships dumbwaiters witon paints paints paints satavahanas satavahanas montevideo luxuries prevailed montevideo prevailed prevailed analogies leto leto leto valens starve complaining repress numbe statistical shots valens vetoes rockerfeller novels novels novels leto novels leto novels leto nassir leto novels leto nassir leto novels leto novels leto nassir leto novels leto novels leto nassir leto synthesized cabildo novels novels leto novels leto '"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adKdcLBbwV5s"
      },
      "source": [
        "1squad = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/train.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "Qi9x1SpNwmJk",
        "outputId": "d2603c50-8327-4ccd-e342-603dcf0ca312"
      },
      "source": [
        "squad.iloc[298,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'On December 13, 2013, Beyoncé unexpectedly released her eponymous fifth studio album on the iTunes Store without any prior announcement or promotion. The album debuted atop the Billboard 200 chart, giving Beyoncé her fifth consecutive number-one album in the US. This made her the first woman in the chart\\'s history to have her first five studio albums debut at number one. Beyoncé received critical acclaim and commercial success, selling one million digital copies worldwide in six days; The New York Times noted the album\\'s unconventional, unexpected release as significant. Musically an electro-R&B album, it concerns darker themes previously unexplored in her work, such as \"bulimia, postnatal depression [and] the fears and insecurities of marriage and motherhood\". The single \"Drunk in Love\", featuring Jay Z, peaked at number two on the Billboard Hot 100 chart. In April 2014, after much speculation in the weeks before, Beyoncé and Jay Z officially announced their On the Run Tour. It served as the couple\\'s first co-headlining stadium tour together. On August 24, 2014, she received the Video Vanguard Award at the 2014 MTV Video Music Awards. Knowles also took home three competitive awards: Best Video with a Social Message and Best Cinematography for \"Pretty Hurts\", as well as best collaboration for \"Drunk in Love\". In November, Forbes reported that Beyoncé was the top-earning woman in music for the second year in a row—earning $115 million in the year, more than double her earnings in 2013. Beyoncé was reissued with new material in three forms: as an extended play, a box set, as well as a full platinum edition.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "WaNcPjR5ww_t",
        "outputId": "2e5be626-87ed-4348-fe05-b5f379409749"
      },
      "source": [
        "squad.iloc[60893,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In 1992, Madonna had a role in A League of Their Own as Mae Mordabito, a baseball player on an all-women\\'s team. She recorded the film\\'s theme song, \"This Used to Be My Playground\", which became a Hot 100 number one hit. The same year, she founded her own entertainment company, Maverick, consisting of a record company (Maverick Records), a film production company (Maverick Films), and associated music publishing, television broadcasting, book publishing and merchandising divisions. The deal was a joint venture with Time Warner and paid Madonna an advance of $60 million. It gave her 20% royalties from the music proceedings, one of the highest rates in the industry, equaled at that time only by Michael Jackson\\'s royalty rate established a year earlier with Sony. The first release from the venture was Madonna\\'s book, titled Sex. It consisted of sexually provocative and explicit images, photographed by Steven Meisel. The book received strong negative reaction from the media and the general public, but sold 1.5 million copies at $50 each in a matter of days. At the same time she released her fifth studio album, Erotica, which debuted at number two on the Billboard 200. Its title track peaked at number three on the Billboard Hot 100. Erotica also produced five singles: \"Deeper and Deeper\", \"Bad Girl\", \"Fever\", \"Rain\" and \"Bye Bye Baby\". Madonna had provocative imagery featured in the erotic thriller, Body of Evidence, a film which contained scenes of sadomasochism and bondage. It was poorly received by critics. She also starred in the film Dangerous Game, which was released straight to video in North America. The New York Times described the film as \"angry and painful, and the pain feels real.\"'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uJL5AQMGw1fo",
        "outputId": "22fba8f0-eb61-4d84-fb6f-8adfdeb49fa5"
      },
      "source": [
        "squad.iloc[60893,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What was the name of the erotic thriller that shows scenes of sadomasochism and bondage?'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cDQKstVdUNn7",
        "outputId": "dc0268c6-26c2-4a76-ae77-de7b7a93cb77"
      },
      "source": [
        "translate(df[\"given_context\"][0], df[\"reference_context\"][0],encoder1,encoder2,decoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what was the name of the film that madonna directed ? <end> '"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dIZgRHgYiZrx",
        "outputId": "66c74120-e927-4832-ad0a-6bb94c857da4"
      },
      "source": [
        "df[\"given_context\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. \""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "DohlGMCBibuz",
        "outputId": "6bd73df0-dbab-4a52-ebdb-307221600fdc"
      },
      "source": [
        "df[\"reference_context\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". '"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnxUIivOMfwq"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-3Bcev6bH4Q"
      },
      "source": [
        "newdf = df.drop_duplicates(\n",
        "  subset = ['given_context', 'reference_context'],\n",
        "  keep = 'first').reset_index(drop = True)\n",
        "  \n",
        "# print latest dataframe\n",
        "display(newdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhrgDlMkbH2I"
      },
      "source": [
        "newdf.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vYXrnKxnitH",
        "outputId": "cac82c19-cb34-402a-e7a5-1dafda79a072"
      },
      "source": [
        "# BLEU SCORE\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = [['this',\"is\",\"a\",\"good\",\"day\"]]\n",
        "candidate = [['hey',\"hi\",\"bad\"],[\"its\",\"not\"]]\n",
        "score = sentence_bleu(ref, given, weights=(1, 0, 0, 0))\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WiK-LAst6b0"
      },
      "source": [
        "ref = [x.split() for x in grp.iloc[0,1]]\n",
        "given = ref[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGdf4aObuNdz",
        "outputId": "30a6688d-523d-4598-ff02-e5010ded324d"
      },
      "source": [
        "given"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['When', 'did', 'Beyonce', 'start', 'becoming', 'popular?']"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sViiZj2JMxgX"
      },
      "source": [
        "squad = pd.read_excel(\"/content/drive/MyDrive/AutomatedQuestionGeneration/train.xlsx\").drop(\"Unnamed: 0\",axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDsycQ4Vpkpp"
      },
      "source": [
        "squad = squad[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIglNlz7qJk0"
      },
      "source": [
        "grp = squad.groupby('context',sort=False).agg(new=('question', list)).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1GrVcBt1B8"
      },
      "source": [
        "def create_reference_dataset_bleu(df):\n",
        "  # Takes the SQUAD dataset\n",
        "  # Groups by \"context\"\n",
        "  # For every unique contxet, a list of all the questions in the squad dataset is created\n",
        "  grp = df.groupby('context',sort=False).agg(new=('question', list)).reset_index()\n",
        "  return grp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p7FWRqd0cR4"
      },
      "source": [
        "def create_candidate_dataset_bleu(df):\n",
        "  # Takes the SQUAD dataset\n",
        "  # Groups by \"context\"\n",
        "  # For every unique contxet, a list of all the questions in the squad dataset is created\n",
        "  grp = df.groupby('context',sort=False).agg(new=('question', list)).reset_index()\n",
        "  return grp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw2FAo5JycrB",
        "outputId": "b4affe44-447f-405c-d0b8-2e768e8db95d"
      },
      "source": [
        "# Loading Trained Model\n",
        "attention1 = BahdanauAttention(units)\n",
        "attention2 = BahdanauAttention(units)\n",
        "encoder1 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "encoder2 = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = DecoderWithDualAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, None, None)\n",
        "encoder1.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder1-6\",)\n",
        "encoder2.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/encoder2-6\")\n",
        "decoder.load_weights(\"/content/drive/MyDrive/AutomatedQuestionGeneration/trained-model/DualAtttentionLSTMBased/decoder6\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f20f6dec090>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFJt1Gs9xK0R"
      },
      "source": [
        "def calculate_bleu(df):\n",
        "  reference = []\n",
        "  data = create_reference_dataset_bleu(df)\n",
        "  bleu = 0\n",
        "  for i in range(len(data[:2])):\n",
        "    reference = [x.split() for x in grp.iloc[i,1]]\n",
        "    print(len(data))\n",
        "    print(len(df))\n",
        "  return ref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7BuFQ_DzuPs",
        "outputId": "4ab3b396-32ef-4015-9e19-85d2fe02e329"
      },
      "source": [
        "len(create_dataset_bleu(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18877"
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyczWcTzx0Ct"
      },
      "source": [
        "calculate_bleu(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "C5DMp0EwsarW",
        "outputId": "0028d2d7-9f3d-43f3-f9f1-c486311216e7"
      },
      "source": [
        "grp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>[When did Beyonce start becoming popular?, Wha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Following the disbandment of Destiny's Child i...</td>\n",
              "      <td>[After her second solo album, what other enter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
              "      <td>[In her music, what are some recurring element...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beyoncé Giselle Knowles was born in Houston, T...</td>\n",
              "      <td>[Beyonce's younger sibling also sang with her ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beyoncé attended St. Mary's Elementary School ...</td>\n",
              "      <td>[What town did Beyonce go to school in?, Who w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context                                                new\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  [When did Beyonce start becoming popular?, Wha...\n",
              "1  Following the disbandment of Destiny's Child i...  [After her second solo album, what other enter...\n",
              "2  A self-described \"modern-day feminist\", Beyonc...  [In her music, what are some recurring element...\n",
              "3  Beyoncé Giselle Knowles was born in Houston, T...  [Beyonce's younger sibling also sang with her ...\n",
              "4  Beyoncé attended St. Mary's Elementary School ...  [What town did Beyonce go to school in?, Who w..."
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "5SgkyYG9o0Uv",
        "outputId": "7ae76ce8-cde7-4cc5-91a0-2866bab2623c"
      },
      "source": [
        "squad.groupby('context').apply(f).iloc[2][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Beyonce's younger sibling also sang with her in what band?Where did Beyonce get her name from?What race was Beyonce's father?Beyonce's childhood home believed in what religion?Beyonce's father worked as a sales manager for what company?Beyonce's mother worked in what industry?What younger sister of Beyonce also appeared in Destiny's Child?Beyonce is a descendent of what Arcadian leader?What company did Beyoncé's father work for when she was a child?What did Beyoncé's mother own when Beyoncé was a child?What is the name of Beyoncé's younger sister?Beyoncé is a descendant of which Acadian leader?Beyoncé was raised in what religion?\""
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTah5G9KtimQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5SSCuX0KOw"
      },
      "source": [
        "### Doing it for a simple example batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjGlZkRj0NuC"
      },
      "source": [
        "# attention = BahdanauAttention(units)\n",
        "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# decoder = DecoderWithAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention)\n",
        "# loss = 0\n",
        "# with tf.GradientTape() as tape: # for automatic differentiation\n",
        "#     sample_hidden = encoder.initialize_hidden_state()\n",
        "#     enc_output, enc_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "#     dec_hidden = enc_hidden\n",
        "#     dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "#     # Teacher forcing - feeding the target as the next input\n",
        "#     for t in range(1, example_target_batch.shape[1]):\n",
        "#         # passing enc_output to the decoder\n",
        "#         predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "#         loss += loss_function(example_target_batch[:, t], predictions)\n",
        "#         # using teacher forcing\n",
        "#         dec_input = tf.expand_dims(example_target_batch[:, t], 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}